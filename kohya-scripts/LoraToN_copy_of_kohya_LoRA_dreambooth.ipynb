{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slgjeYgd6pWp"
   },
   "source": [
    "![visitors](https://visitor-badge.glitch.me/badge?page_id=linaqruf.lora-dreambooth)\n",
    "\n",
    "# Kohya LoRA Dreambooth<br><small><small>A Colab Notebook For LoRA Training (Dreambooth Method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPgBR3KM6E-Z"
   },
   "source": [
    "Adapted to Google Colab based on [kohya-ss/sd-script](https://github.com/kohya-ss/sd-scripts)<br>\n",
    "Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)<br>\n",
    "You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_6E01tRBfQU"
   },
   "source": [
    "| Notebook Name | Description | Link |\n",
    "| --- | --- | --- |\n",
    "| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) |\n",
    "| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) |\n",
    "| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) |\n",
    "| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=for-the-badge)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) |\n",
    "| Kohya Textual Inversion  | Textual Inversion Training | SOON |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTVqCAgSmie4"
   },
   "source": [
    "# I. Install Kohya Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickleshare in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (0.7.5)\n",
      "Requirement already satisfied: accelerate in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: google in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: prettytable in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from google) (4.12.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from prettytable) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from torch>=1.4.0->accelerate) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from torch>=1.4.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from torch>=1.4.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from torch>=1.4.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from torch>=1.4.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from torch>=1.4.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from beautifulsoup4->google) (2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pickleshare accelerate google prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_u3q60di584x",
    "outputId": "a6b59c0a-ffe9-4f76-b48d-f8d5dba220ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 21 14:13:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.33                 Driver Version: 546.33       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:05:00.0  On |                  N/A |\n",
      "|  0%   51C    P8              16W / 170W |   1025MiB / 12288MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1208    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      4256    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6500    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      8232    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9900    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10084    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10536    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     11328    C+G   ...63.0_x64__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     11680    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11696    C+G   ...on\\123.0.2420.97\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     12292    C+G   ...al\\Discord\\app-1.0.9042\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     12564    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     12664    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13848    C+G   ...Brave-Browser\\Application\\brave.exe    N/A      |\n",
      "|    0   N/A  N/A     14184    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     16412    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Stored 'root_dir' (str)\n",
      "Stored 'repo_dir' (str)\n",
      "Stored 'tools_dir' (str)\n",
      "Stored 'finetune_dir' (str)\n",
      "Stored 'training_dir' (str)\n",
      "c:\\content\n"
     ]
    }
   ],
   "source": [
    "#@title ## 1.1. Clone Kohya Trainer\n",
    "#@markdown Clone Kohya Trainer from GitHub and check for updates. Use textbox below if you want to checkout other branch or old commit. Leave it empty to stay the HEAD on main.\n",
    "\n",
    "import os\n",
    "# %store -r\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "root_dir = \"/content\"\n",
    "%store root_dir\n",
    "repo_dir = str(root_dir)+\"/kohya-trainer\"\n",
    "%store repo_dir\n",
    "tools_dir = str(root_dir)+\"/kohya-trainer/tools\"\n",
    "%store tools_dir\n",
    "finetune_dir = str(root_dir)+\"/kohya-trainer/finetune\"\n",
    "%store finetune_dir\n",
    "training_dir = str(root_dir)+\"/dreambooth\"\n",
    "%store training_dir\n",
    "\n",
    "branch = \"\" #@param {type: \"string\"}\n",
    "repo_url = \"https://github.com/Linaqruf/kohya-trainer\"\n",
    "\n",
    "def clone_repo():\n",
    "  if os.path.isdir(repo_dir):\n",
    "    print(\"The repository folder already exists, will do a !git pull instead\\n\")\n",
    "    %cd {repo_dir}\n",
    "    !git pull origin {branch} if branch else !git pull\n",
    "  else:\n",
    "    %cd {root_dir}\n",
    "    !git clone {repo_url} {repo_dir}\n",
    "\n",
    "if not os.path.isdir(repo_dir):\n",
    "  clone_repo()\n",
    "\n",
    "%cd {root_dir}\n",
    "os.makedirs(repo_dir, exist_ok=True)\n",
    "os.makedirs(tools_dir, exist_ok=True)\n",
    "os.makedirs(finetune_dir, exist_ok=True)\n",
    "os.makedirs(training_dir, exist_ok=True)\n",
    "\n",
    "if branch:\n",
    "  %cd {repo_dir}\n",
    "  status = os.system(f\"git checkout {branch}\")\n",
    "  if status != 0:\n",
    "    raise Exception(\"Failed to checkout branch or commit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WNn0g1pnHfk5",
    "outputId": "87cd3fa5-d093-408c-891e-a79532a06e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\content\\kohya-trainer\n",
      "Stored 'accelerate_config' (str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "ERROR: xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl is not a supported wheel on this platform.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Collecting jax==0.4.23 (from jax[cuda12_pip]==0.4.23)\n",
      "  Using cached jax-0.4.23-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (1.13.0)\n",
      "INFO: pip is looking at multiple versions of jax[cuda12-pip] to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement jaxlib==0.4.23+cuda12.cudnn89; extra == \"cuda12_pip\" (from jax[cuda12-pip]) (from versions: 0.4.13, 0.4.14, 0.4.16, 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.25, 0.4.26)\n",
      "ERROR: No matching distribution found for jaxlib==0.4.23+cuda12.cudnn89; extra == \"cuda12_pip\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flax in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (1.26.4)\n",
      "Requirement already satisfied: jax>=0.4.19 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (0.4.26)\n",
      "Requirement already satisfied: msgpack in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (1.0.8)\n",
      "Requirement already satisfied: optax in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (0.2.2)\n",
      "Requirement already satisfied: orbax-checkpoint in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (0.5.9)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (0.1.56)\n",
      "Requirement already satisfied: rich>=11.1 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (13.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from flax) (6.0.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax>=0.4.19->flax) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax>=0.4.19->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from jax>=0.4.19->flax) (1.13.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from rich>=11.1->flax) (2.17.2)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from optax->flax) (2.1.0)\n",
      "Requirement already satisfied: chex>=0.1.86 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from optax->flax) (0.1.86)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from optax->flax) (0.4.26)\n",
      "Requirement already satisfied: etils[epath,epy] in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from orbax-checkpoint->flax) (1.8.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from orbax-checkpoint->flax) (4.25.3)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\jerico rangel\\miniconda3\\envs\\lora-env\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.18.1)\n"
     ]
    }
   ],
   "source": [
    "#@title ## 1.2. Installing Dependencies\n",
    "#@markdown This will install required Python packages\n",
    "import os\n",
    "# %store -r\n",
    "\n",
    "%cd {repo_dir}\n",
    "\n",
    "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
    "%store accelerate_config\n",
    "install_xformers = True #@param {'type':'boolean'}\n",
    "\n",
    "def install_dependencies():\n",
    "    !python -m pip -q install --upgrade gallery-dl gdown imjoy-elfinder\n",
    "    !apt -q install liblz4-tool aria2\n",
    "    !python -m pip -q install --upgrade -r requirements.txt\n",
    "\n",
    "    if install_xformers:\n",
    "        !python -m pip -q install -U -I --no-deps https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "    from accelerate.utils import write_basic_config\n",
    "    if not os.path.exists(accelerate_config):\n",
    "        write_basic_config(save_location=accelerate_config)\n",
    "\n",
    "    !python -m pip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "    !python -m pip install -U flax\n",
    "\n",
    "install_dependencies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt9EJv5gQXuB"
   },
   "source": [
    "## 1.3. Sign-in to Cloud Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rl2zERHbBQ9W",
    "outputId": "53c5ab70-34e4-4f7b-caa9-74efcb9fc2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Jerico Rangel\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Stored 'write_token' (str)\n"
     ]
    }
   ],
   "source": [
    "#@title ### 1.3.1. Login to Huggingface hub\n",
    "from huggingface_hub import login\n",
    "# %store -r\n",
    "\n",
    "#@markdown Login to Huggingface hub\n",
    "#@markdown 1. You need a Huggingface account.\n",
    "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
    "write_token = \"*******\" #@param {type:\"string\"}\n",
    "login(write_token, add_to_git_credential=True)\n",
    "\n",
    "%store write_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gob9_OwTlwh"
   },
   "source": [
    "# II. Pretrained Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmnsZwClN1XL",
    "outputId": "80129105-0025-48ee-e6f4-4d037cf0bf55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'aria2c' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#@title ## 2.1. Download Available Model\n",
    "import os\n",
    "# %store -r\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "installModels = []\n",
    "installv2Models = []\n",
    "\n",
    "#@markdown ### Available Model\n",
    "#@markdown Select one of available model to download:\n",
    "\n",
    "#@markdown ### SD1.x model\n",
    "modelUrl = [\"\", \\\n",
    "            \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/models/animefull-final-pruned.ckpt\", \\\n",
    "            \"https://huggingface.co/cag/anything-v3-1/resolve/main/anything-v3-1.safetensors\", \\\n",
    "            \"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.ckpt\", \\\n",
    "            \"https://huggingface.co/Rasgeath/self_made_sauce/resolve/main/Kani-anime-pruned.ckpt\", \\\n",
    "            \"https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors\", \\\n",
    "            \"https://huggingface.co/gsdf/Counterfeit-V2.0/resolve/main/Counterfeit-V2.0fp16.safetensors\", \\\n",
    "            \"https://huggingface.co/closertodeath/dpepteahands3/resolve/main/dpepteahand3.ckpt\", \\\n",
    "            \"https://huggingface.co/prompthero/openjourney-v2/resolve/main/openjourney-v2.ckpt\", \\\n",
    "            \"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt\", \\\n",
    "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\"]\n",
    "modelList = [\"\", \\\n",
    "             \"Animefull-final-pruned\", \\\n",
    "             \"Anything-v3-1\", \\\n",
    "             \"Anything-v4-5-pruned\", \\\n",
    "             \"Kani-anime-pruned\", \\\n",
    "             \"AbyssOrangeMix2-nsfw\", \\\n",
    "             \"Counterfeit-v2\", \\\n",
    "             \"DpepTeaHands3\", \\\n",
    "             \"OpenJourney-v2\", \\\n",
    "             \"Dreamlike-diffusion-v1-0\", \\\n",
    "             \"Stable-Diffusion-v1-5\"]\n",
    "modelName = \"Stable-Diffusion-v1-5\"  #@param [\"\", \"Animefull-final-pruned\", \"Anything-v3-1\", \"Anything-v4-5-pruned\", \"Kani-anime-pruned\", \"AbyssOrangeMix2-nsfw\", \"Counterfeit-v2\", \"DpepTeaHands3\", \"OpenJourney-v2\", \"Dreamlike-diffusion-v1-0\", \"Stable-Diffusion-v1-5\"]\n",
    "\n",
    "#@markdown ### SD2.x model\n",
    "v2ModelUrl = [\"\", \\\n",
    "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.ckpt\", \\\n",
    "              \"https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.ckpt\", \\\n",
    "              \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e2.ckpt\", \\\n",
    "              \"https://huggingface.co/p1atdev/pd-archive/resolve/main/plat-v1-3-1.safetensors\"]\n",
    "v2ModelList = [\"\", \\\n",
    "              \"stable-diffusion-2-1-base\", \\\n",
    "              \"stable-diffusion-2-1-768v\", \\\n",
    "              \"waifu-diffusion-1-4-anime-e2\", \\\n",
    "              \"plat-diffusion-v1-3-1\"]\n",
    "v2ModelName = \"\" #@param [\"\", \"stable-diffusion-2-1-base\", \"stable-diffusion-2-1-768v\", \"waifu-diffusion-1-4-anime-e2\", \"plat-diffusion-v1-3-1\"]\n",
    "\n",
    "if modelName != \"\":\n",
    "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
    "if v2ModelName != \"\":\n",
    "  installv2Models.append((v2ModelName, v2ModelUrl[v2ModelList.index(v2ModelName)]))\n",
    "\n",
    "def install(checkpoint_name, url):\n",
    "  ext = \"ckpt\" if url.endswith(\".ckpt\") else \"safetensors\"\n",
    "\n",
    "  hf_token = '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'\n",
    "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
    "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {root_dir}/pre_trained_model -o {checkpoint_name}.{ext} \"{url}\"\n",
    "\n",
    "def install_checkpoint():\n",
    "  for model in installModels:\n",
    "    install(model[0], model[1])\n",
    "  for v2model in installv2Models:\n",
    "    install(v2model[0], v2model[1])\n",
    "\n",
    "install_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "SoucgZQ6jgPQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'aria2c' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#@title ## 2.3. Download Available VAE (Optional)\n",
    "# %store -r\n",
    "\n",
    "%cd {root_dir}\n",
    "\n",
    "installVae = []\n",
    "#@markdown ### Available VAE\n",
    "#@markdown Select one of the VAEs to download, select `none` for not download VAE:\n",
    "vaeUrl = [\"\", \\\n",
    "          \"https://huggingface.co/Linaqruf/personal-backup/resolve/main/vae/animevae.pt\", \\\n",
    "          \"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt\", \\\n",
    "          \"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\"]\n",
    "vaeList = [\"none\", \\\n",
    "           \"anime.vae.pt\", \\\n",
    "           \"waifudiffusion.vae.pt\", \\\n",
    "           \"stablediffusion.vae.pt\"]\n",
    "vaeName = \"waifudiffusion.vae.pt\" #@param [\"none\", \"anime.vae.pt\", \"waifudiffusion.vae.pt\", \"stablediffusion.vae.pt\"]\n",
    "\n",
    "installVae.append((vaeName, vaeUrl[vaeList.index(vaeName)]))\n",
    "\n",
    "def install(vae_name, url):\n",
    "  hf_token = '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'\n",
    "  user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
    "  !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -o vae/{vae_name} \"{url}\"\n",
    "\n",
    "def install_vae():\n",
    "  if vaeName != \"none\":\n",
    "    for vae in installVae:\n",
    "      install(vae[0], vae[1])\n",
    "  else:\n",
    "    pass\n",
    "\n",
    "install_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHNbl3O_NSS0"
   },
   "source": [
    "# V. Training Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "H_Q23fUEJhnC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_folder_directory' (str)\n",
      "Stored 'reg_folder_directory' (str)\n"
     ]
    }
   ],
   "source": [
    "#@title ## 5.1. Define Important folder\n",
    "# from google.colab import drive\n",
    "# %store -r\n",
    "\n",
    "v2 = False #@param {type:\"boolean\"}\n",
    "v_parameterization = False #@param {type:\"boolean\"}\n",
    "project_name = \"LoRA-Ton\" #@param {type:\"string\"}\n",
    "pretrained_model_name_or_path = \"/c/content/downloads/v1-5-pruned-emaonly.ckpt\" #@param {type:\"string\"}\n",
    "vae = \"/c/content/downloads/kl-f8-anime.ckpt\"  #@param {type:\"string\"}\n",
    "#@markdown You need to register parent folder and not where `train_data_dir` located\n",
    "train_folder_directory = \"/c/content/Wellington\" #@param {'type':'string'}\n",
    "%store train_folder_directory\n",
    "reg_folder_directory = \"/c/content/Wellington\" #@param {'type':'string'}\n",
    "%store reg_folder_directory\n",
    "output_dir = \"/c/content/outputs\" #@param {'type':'string'}\n",
    "resume_path =\"\"\n",
    "inference_url = \"https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/\"\n",
    "\n",
    "#@markdown This will ignore `output_dir` defined above, and changed to `/content/drive/MyDrive/fine_tune/output` by default\n",
    "output_to_drive = False #@param {'type':'boolean'}\n",
    "\n",
    "if output_to_drive:\n",
    "  output_dir = \"/content/drive/MyDrive/dreambooth/output\"\n",
    "\n",
    "  if not os.path.exists(\"/content/drive\"):\n",
    "    pass\n",
    "    # drive.mount('/content/drive')\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if v2 and not v_parameterization:\n",
    "  inference_url += \"v2-inference.yaml\"\n",
    "if v2 and v_parameterization:\n",
    "  inference_url += \"v2-inference-v.yaml\"\n",
    "\n",
    "try:\n",
    "  if v2:\n",
    "    !wget {inference_url} -O {output_dir}/{project_name}.yaml\n",
    "    print(\"File successfully downloaded\")\n",
    "except:\n",
    "  print(\"There was an error downloading the file. Please check the URL and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "form",
    "id": "5P-QVvHMUrFB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network module: networks.lora\n",
      "networks.lora dim set to: 128\n",
      "networks.lora alpha set to: 128\n",
      "No LoRA weight loaded.\n",
      "Global learning rate:  0.0001\n",
      "Enabling LoRA for U-Net\n",
      "Enabling LoRA for Text Encoder\n",
      "Text encoder learning rate:  5e-05\n",
      "Learning rate Scheduler: constant\n",
      "Training comment: this_comment_will_be_stored_in_the_metadata\n"
     ]
    }
   ],
   "source": [
    "#@title ## 5.2. Define Specific LoRA Training Parameters\n",
    "# %store -r\n",
    "\n",
    "#@markdown ## LoRA - Low Rank Adaptation Dreambooth\n",
    "\n",
    "#@markdown Some people recommend setting the `network_dim` to a higher value.\n",
    "network_dim = 128 #@param {'type':'number'}\n",
    "#@markdown For weight scaling in LoRA, it is better to set `network_alpha` the same as `network_dim` unless you know what you're doing. A lower `network_alpha` requires a higher learning rate. For example, if `network_alpha = 1`, then `unet_lr = 1e-3`.\n",
    "network_alpha = 128 #@param {'type':'number'}\n",
    "network_module = \"networks.lora\"\n",
    "\n",
    "#@markdown `network_weights` can be specified to resume training.\n",
    "network_weights = \"\" #@param {'type':'string'}\n",
    "\n",
    "#@markdown By default, both Text Encoder and U-Net LoRA modules are enabled. Use `network_train_on` to specify which module to train.\n",
    "network_train_on = \"both\" #@param ['both','unet_only', 'text_encoder_only'] {'type':'string'}\n",
    "\n",
    "#@markdown It is recommended to set the `text_encoder_lr` to a lower learning rate, such as `5e-5`, or to set `text_encoder_lr = 1/2 * unet_lr`.\n",
    "learning_rate = 1e-4 #@param {'type':'number'}\n",
    "unet_lr = 0 #@param {'type':'number'}\n",
    "text_encoder_lr = 5e-5 #@param {'type':'number'}\n",
    "lr_scheduler = \"constant\" #@param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] {allow-input: false}\n",
    "\n",
    "#@markdown If `lr_scheduler = cosine_with_restarts`, update `lr_scheduler_num_cycles`.\n",
    "lr_scheduler_num_cycles = 1 #@param {'type':'number'}\n",
    "#@markdown If `lr_scheduler = polynomial`, update `lr_scheduler_power`.\n",
    "lr_scheduler_power = 1 #@param {'type':'number'}\n",
    "\n",
    "#@markdown Check the box to not save metadata in the output model.\n",
    "no_metadata = False #@param {type:\"boolean\"}\n",
    "training_comment = \"this comment will be stored in the metadata\" #@param {'type':'string'}\n",
    "\n",
    "print(\"Loading network module:\", network_module)\n",
    "print(f\"{network_module} dim set to:\", network_dim)\n",
    "print(f\"{network_module} alpha set to:\", network_alpha)\n",
    "\n",
    "if network_weights == \"\":\n",
    "  print(\"No LoRA weight loaded.\")\n",
    "else:\n",
    "  if os.path.exists(network_weights):\n",
    "    print(\"Loading LoRA weight:\", network_weights)\n",
    "  else:\n",
    "    print(f\"{network_weights} does not exist.\")\n",
    "    network_weights =\"\"\n",
    "\n",
    "if network_train_on == \"unet_only\":\n",
    "  print(\"Enabling LoRA for U-Net.\")\n",
    "  print(\"Disabling LoRA for Text Encoder.\")\n",
    "\n",
    "print(\"Global learning rate: \", learning_rate)\n",
    "\n",
    "if network_train_on == \"unet_only\":\n",
    "  print(\"Enable LoRA for U-Net\")\n",
    "  print(\"Disable LoRA for Text Encoder\")\n",
    "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
    "if network_train_on == \"text_encoder_only\":\n",
    "  print(\"Disabling LoRA for U-Net\")\n",
    "  print(\"Enabling LoRA for Text Encoder\")\n",
    "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
    "else:\n",
    "  print(\"Enabling LoRA for U-Net\")\n",
    "  print(\"Enabling LoRA for Text Encoder\")\n",
    "  print(\"UNet learning rate: \", unet_lr) if unet_lr != 0 else \"\"\n",
    "  print(\"Text encoder learning rate: \", text_encoder_lr) if text_encoder_lr != 0 else \"\"\n",
    "\n",
    "print(\"Learning rate Scheduler:\", lr_scheduler)\n",
    "\n",
    "if lr_scheduler == \"cosine_with_restarts\":\n",
    "  print(\"- Number of cycles: \", lr_scheduler_num_cycles)\n",
    "elif lr_scheduler == \"polynomial\":\n",
    "  print(\"- Power: \", lr_scheduler_power)\n",
    "\n",
    "# Printing the training comment if metadata is not disabled and a comment is present\n",
    "if not no_metadata:\n",
    "  if training_comment:\n",
    "    training_comment = training_comment.replace(\" \", \"_\")\n",
    "    print(\"Training comment:\", training_comment)\n",
    "else:\n",
    "  print(\"Metadata won't be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "GIvfcS1pOrGY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\content\\kohya-trainer\n",
      "+-------------------------------+-----------------------------------------------+\n",
      "| Hyperparameter                | Value                                         |\n",
      "+-------------------------------+-----------------------------------------------+\n",
      "| v2                            | False                                         |\n",
      "| v_parameterization            | False                                         |\n",
      "| network_dim                   | 128                                           |\n",
      "| network_alpha                 | 128                                           |\n",
      "| network_module                | networks.lora                                 |\n",
      "| network_weights               | False                                         |\n",
      "| network_train_on              | both                                          |\n",
      "| learning_rate                 | 0.0001                                        |\n",
      "| unet_lr                       | 0                                             |\n",
      "| text_encoder_lr               | 5e-05                                         |\n",
      "| no_metadata                   | False                                         |\n",
      "| training_comment              | this_comment_will_be_stored_in_the_metadata   |\n",
      "| lr_scheduler                  | constant                                      |\n",
      "| lr_scheduler_num_cycles       | 1                                             |\n",
      "| lr_scheduler_power            | 1                                             |\n",
      "| pretrained_model_name_or_path | /c/content/downloads/v1-5-pruned-emaonly.ckpt |\n",
      "| vae                           | /c/content/downloads/kl-f8-anime.ckpt         |\n",
      "| caption_extension             | .txt                                          |\n",
      "| train_folder_directory        | /c/content/Wellington                         |\n",
      "| reg_folder_directory          | /c/content/Wellington                         |\n",
      "| output_dir                    | /c/content/outputs                            |\n",
      "| prior_loss_weight             | 1.0                                           |\n",
      "| resume_path                   | False                                         |\n",
      "| project_name                  | LoRA-Ton                                      |\n",
      "| mixed_precision               | fp16                                          |\n",
      "| save_precision                | fp16                                          |\n",
      "| save_n_epochs_type            | save_n_epoch_ratio                            |\n",
      "| save_n_epochs_type_value      | 3                                             |\n",
      "| save_model_as                 | safetensors                                   |\n",
      "| resolution                    | 512                                           |\n",
      "| enable_bucket                 | True                                          |\n",
      "| min_bucket_reso               | 256                                           |\n",
      "| max_bucket_reso               | 1024                                          |\n",
      "| cache_latents                 | True                                          |\n",
      "| train_batch_size              | 6                                             |\n",
      "| max_token_length              | 225                                           |\n",
      "| use_8bit_adam                 | True                                          |\n",
      "| num_epochs                    | 20                                            |\n",
      "| seed                          | 0                                             |\n",
      "| gradient_checkpointing        | False                                         |\n",
      "| gradient_accumulation_steps   | 1                                             |\n",
      "| clip_skip                     | 2                                             |\n",
      "| logging_dir                   | /dreambooth/logs                              |\n",
      "| log_prefix                    | LoRA-Ton                                      |\n",
      "| additional_argument           | --shuffle_caption --xformers                  |\n",
      "+-------------------------------+-----------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "import textwrap\n",
    "import yaml\n",
    "\n",
    "# %store -r\n",
    "\n",
    "#@title ## 5.3. Start LoRA Dreambooth\n",
    "#@markdown ### Define Parameter\n",
    "\n",
    "train_batch_size = 6 #@param {type:\"number\"}\n",
    "num_epochs = 20 #@param {type:\"number\"}\n",
    "caption_extension = '.txt' #@param {'type':'string'}\n",
    "mixed_precision = \"fp16\" #@param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
    "save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
    "save_n_epochs_type = \"save_n_epoch_ratio\" #@param [\"save_every_n_epochs\", \"save_n_epoch_ratio\"] {allow-input: false}\n",
    "save_n_epochs_type_value = 3 #@param {type:\"number\"}\n",
    "save_model_as = \"safetensors\" #@param [\"ckpt\", \"pt\", \"safetensors\"] {allow-input: false}\n",
    "resolution = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
    "enable_bucket = True #@param {type:\"boolean\"}\n",
    "min_bucket_reso = 320 if resolution > 640 else 256\n",
    "max_bucket_reso = 1280 if resolution > 640 else 1024\n",
    "cache_latents = True #@param {type:\"boolean\"}\n",
    "max_token_length = 225 #@param {type:\"number\"}\n",
    "clip_skip = 2 #@param {type:\"number\"}\n",
    "use_8bit_adam = True #@param {type:\"boolean\"}\n",
    "gradient_checkpointing = False #@param {type:\"boolean\"}\n",
    "gradient_accumulation_steps = 1 #@param {type:\"number\"}\n",
    "seed = 0 #@param {type:\"number\"}\n",
    "logging_dir = \"/dreambooth/logs\"\n",
    "log_prefix = project_name\n",
    "additional_argument = \"--shuffle_caption --xformers\" #@param {type:\"string\"}\n",
    "print_hyperparameter = True #@param {type:\"boolean\"}\n",
    "prior_loss_weight = 1.0\n",
    "%cd {repo_dir}\n",
    "\n",
    "train_command=f\"\"\"\n",
    "accelerate launch --config_file={accelerate_config} --num_cpu_threads_per_process=8 train_network.py \\\n",
    "  {\"--v2\" if v2 else \"\"} \\\n",
    "  {\"--v_parameterization\" if v2 and v_parameterization else \"\"} \\\n",
    "  --network_dim={network_dim} \\\n",
    "  --network_alpha={network_alpha} \\\n",
    "  --network_module={network_module} \\\n",
    "  {\"--network_weights=\" + network_weights if network_weights else \"\"} \\\n",
    "  {\"--network_train_unet_only\" if network_train_on == \"unet_only\" else \"\"} \\\n",
    "  {\"--network_train_text_encoder_only\" if network_train_on == \"text_encoder_only\" else \"\"} \\\n",
    "  --learning_rate={learning_rate} \\\n",
    "  {\"--unet_lr=\" + format(unet_lr) if unet_lr !=0 else \"\"} \\\n",
    "  {\"--text_encoder_lr=\" + format(text_encoder_lr) if text_encoder_lr !=0 else \"\"} \\\n",
    "  {\"--no_metadata\" if no_metadata else \"\"} \\\n",
    "  {\"--training_comment=\" + training_comment if training_comment and not no_metadata else \"\"} \\\n",
    "  --lr_scheduler={lr_scheduler} \\\n",
    "  {\"--lr_scheduler_num_cycles=\" + format(lr_scheduler_num_cycles) if lr_scheduler == \"cosine_with_restarts\" else \"\"} \\\n",
    "  {\"--lr_scheduler_power=\" + format(lr_scheduler_power) if lr_scheduler == \"polynomial\" else \"\"} \\\n",
    "  --pretrained_model_name_or_path={pretrained_model_name_or_path} \\\n",
    "  {\"--vae=\" + vae if vae else \"\"} \\\n",
    "  {\"--caption_extension=\" + caption_extension if caption_extension else \"\"} \\\n",
    "  --train_data_dir={train_folder_directory} \\\n",
    "  --reg_data_dir={reg_folder_directory} \\\n",
    "  --output_dir={output_dir} \\\n",
    "  --prior_loss_weight={prior_loss_weight} \\\n",
    "  {\"--resume=\" + resume_path if resume_path else \"\"} \\\n",
    "  {\"--output_name=\" + project_name if project_name else \"\"} \\\n",
    "  --mixed_precision={mixed_precision} \\\n",
    "  --save_precision={save_precision} \\\n",
    "  {\"--save_every_n_epochs=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_every_n_epochs\" else \"\"} \\\n",
    "  {\"--save_n_epoch_ratio=\" + format(save_n_epochs_type_value) if save_n_epochs_type==\"save_n_epoch_ratio\" else \"\"} \\\n",
    "  --save_model_as={save_model_as} \\\n",
    "  --resolution={resolution} \\\n",
    "  {\"--enable_bucket\" if enable_bucket else \"\"} \\\n",
    "  {\"--min_bucket_reso=\" + format(min_bucket_reso) if enable_bucket else \"\"} \\\n",
    "  {\"--max_bucket_reso=\" + format(max_bucket_reso) if enable_bucket else \"\"} \\\n",
    "  {\"--cache_latents\" if cache_latents else \"\"} \\\n",
    "  --train_batch_size={train_batch_size} \\\n",
    "  --max_token_length={max_token_length} \\\n",
    "  {\"--use_8bit_adam\" if use_8bit_adam else \"\"} \\\n",
    "  --max_train_epochs={num_epochs} \\\n",
    "  {\"--seed=\" + format(seed) if seed > 0 else \"\"} \\\n",
    "  {\"--gradient_checkpointing\" if gradient_checkpointing else \"\"} \\\n",
    "  {\"--gradient_accumulation_steps=\" + format(gradient_accumulation_steps) } \\\n",
    "  {\"--clip_skip=\" + format(clip_skip) if v2 == False else \"\"} \\\n",
    "  --logging_dir={logging_dir} \\\n",
    "  --log_prefix={log_prefix} \\\n",
    "  {additional_argument}\n",
    "  \"\"\"\n",
    "\n",
    "debug_params = [\"v2\", \\\n",
    "                \"v_parameterization\", \\\n",
    "                \"network_dim\", \\\n",
    "                \"network_alpha\", \\\n",
    "                \"network_module\", \\\n",
    "                \"network_weights\", \\\n",
    "                \"network_train_on\", \\\n",
    "                \"learning_rate\", \\\n",
    "                \"unet_lr\", \\\n",
    "                \"text_encoder_lr\", \\\n",
    "                \"no_metadata\", \\\n",
    "                \"training_comment\", \\\n",
    "                \"lr_scheduler\", \\\n",
    "                \"lr_scheduler_num_cycles\", \\\n",
    "                \"lr_scheduler_power\", \\\n",
    "                \"pretrained_model_name_or_path\", \\\n",
    "                \"vae\", \\\n",
    "                \"caption_extension\", \\\n",
    "                \"train_folder_directory\", \\\n",
    "                \"reg_folder_directory\", \\\n",
    "                \"output_dir\", \\\n",
    "                \"prior_loss_weight\", \\\n",
    "                \"resume_path\", \\\n",
    "                \"project_name\", \\\n",
    "                \"mixed_precision\", \\\n",
    "                \"save_precision\", \\\n",
    "                \"save_n_epochs_type\", \\\n",
    "                \"save_n_epochs_type_value\", \\\n",
    "                \"save_model_as\", \\\n",
    "                \"resolution\", \\\n",
    "                \"enable_bucket\", \\\n",
    "                \"min_bucket_reso\", \\\n",
    "                \"max_bucket_reso\", \\\n",
    "                \"cache_latents\", \\\n",
    "                \"train_batch_size\", \\\n",
    "                \"max_token_length\", \\\n",
    "                \"use_8bit_adam\", \\\n",
    "                \"num_epochs\", \\\n",
    "                \"seed\", \\\n",
    "                \"gradient_checkpointing\", \\\n",
    "                \"gradient_accumulation_steps\", \\\n",
    "                \"clip_skip\", \\\n",
    "                \"logging_dir\", \\\n",
    "                \"log_prefix\", \\\n",
    "                \"additional_argument\"]\n",
    "\n",
    "if print_hyperparameter:\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Hyperparameter\", \"Value\"]\n",
    "    for params in debug_params:\n",
    "        if params != \"\":\n",
    "            if globals()[params] == \"\":\n",
    "                value = \"False\"\n",
    "            else:\n",
    "                value = globals()[params]\n",
    "            table.add_row([params, value])\n",
    "    table.align = \"l\"\n",
    "    print(table)\n",
    "\n",
    "    arg_list = train_command.split()\n",
    "    mod_train_command = {'command': arg_list}\n",
    "\n",
    "    train_folder = os.path.dirname(output_dir)\n",
    "\n",
    "    # # save the YAML string to a file\n",
    "    # with open(str(train_folder)+'dreambooth_lora_cmd.yaml', 'w') as f:\n",
    "    #     yaml.dump(mod_train_command, f)\n",
    "\n",
    "f = open(\"./train.sh\", \"w\")\n",
    "f.write(train_command)\n",
    "f.close()\n",
    "!chmod +x ./train.sh\n",
    "!./train.sh"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
